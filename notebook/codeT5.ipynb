{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFUdxiMoNLF7",
    "outputId": "3ff085f2-c64c-40cc-eb32-d2a9d20fa6f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9gUsJW1gNPAA",
    "outputId": "8ccd36ac-7bf3-4736-b5ad-18b7345f47a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pyarrow 16.1.0\n",
      "Uninstalling pyarrow-16.1.0:\n",
      "  Successfully uninstalled pyarrow-16.1.0\n",
      "Found existing installation: requests 2.32.3\n",
      "Uninstalling requests-2.32.3:\n",
      "  Successfully uninstalled requests-2.32.3\n",
      "Requirement already satisfied: transformers in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (4.41.2)\n",
      "Requirement already satisfied: datasets in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: torch in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: evaluate in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (0.4.2)\n",
      "Requirement already satisfied: filelock in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (3.15.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/nm788186/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/nm788186/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: requests, pyarrow\n",
      "Successfully installed pyarrow-16.1.0 requests-2.32.3\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y pyarrow requests\n",
    "\n",
    "%pip install transformers datasets torch evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q16SEx-6NQy-",
    "outputId": "f30ef856-4d4c-4795-acdd-43a5a5466e5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (0.32.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from accelerate) (2.3.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from accelerate) (0.23.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.15.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
      "Requirement already satisfied: networkx in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
      "Requirement already satisfied: requests in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "0.32.1\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate -U\n",
    "import accelerate\n",
    "print(accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZXCLhml6NUTu"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# Load your XGLUE JavaScript dataset\n",
    "# THE DATASET FROM CODE_X_GLUE IS ALREADY PREPROCESSED\n",
    "dataset = load_dataset('code_x_glue_ct_code_to_text', 'javascript')\n",
    "\n",
    "# Convert the dataset to Pandas DataFrames\n",
    "\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "val_df = pd.DataFrame(dataset['validation'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable for CUDA memory allocation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import torch\n",
    "\n",
    "# Your PyTorch code goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "u4-ff1k-Ncp-",
    "outputId": "d7601729-6bbf-443e-af92-c1ad9a9bc05e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (732 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer and model loaded successfully.\n",
      "Available training samples: 54770\n",
      "Available validation samples: 3669\n",
      "Available test samples: 3110\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920aff0e108342a8bebfa3989c73104d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54770 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41cf26302e34e43890652a994c26a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3669 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4a79cf5c334edf857e9095690ece42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058ff610d3d147a5a2907496a2abb528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54770 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442164c4e58a4ecd9a5acb440c238542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3669 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c212151567e4e0589e1a101a3f701ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11984' max='20544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11984/20544 1:35:04 < 1:07:55, 2.10 it/s, Epoch 7/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.496900</td>\n",
       "      <td>0.621673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.505100</td>\n",
       "      <td>0.621418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.497700</td>\n",
       "      <td>0.624025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.467600</td>\n",
       "      <td>0.632316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.463400</td>\n",
       "      <td>0.635072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.642205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.428800</td>\n",
       "      <td>0.648374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [230/230 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6214179396629333, 'eval_runtime': 34.6305, 'eval_samples_per_second': 105.947, 'eval_steps_per_second': 6.642, 'epoch': 7.0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer\n",
    "import torch\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Model name\n",
    "#load the model directly from Salesforce/codeT5 when you are training for the first time\n",
    "#If fine tuning the models again then write the model_name as the previously saved model\n",
    "model_name = \"/data/T5/codeT53_54k\"\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Initialize the model\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "print(\"Tokenizer and model loaded successfully.\") #to verify if the model is loaded succefully or not\n",
    "\n",
    "# Filter the datasets based on code_tokens_length\n",
    "\n",
    "#it filters the data that has the length of code tokens as more than 512 in train data. you can either take code colm(which contain the actual code snippet) and then encode it\n",
    "#  using tokenizer. OR even you can use codetokens colum to directly filter out the thing without encoding. Similarly it is done for test and validation data.\n",
    "train_df['code_tokens_length'] = train_df['code'].apply(lambda x: len(tokenizer.encode(x)))#tokenize the code snippet in train data\n",
    "sampled_train_df = train_df[train_df['code_tokens_length'] <= 512]# filter out the data by removing the data that have more than 512 tokens and store it in sampled_train_df\n",
    "\n",
    "val_df['code_tokens_length'] = val_df['code'].apply(lambda x: len(tokenizer.encode(x)))#same explanation as in train data\n",
    "sampled_val_df = val_df[val_df['code_tokens_length'] <= 512]\n",
    "\n",
    "test_df['code_tokens_length'] = test_df['code'].apply(lambda x: len(tokenizer.encode(x)))\n",
    "sampled_test_df = test_df[test_df['code_tokens_length'] <= 512]\n",
    "\n",
    "# Check the number of available rows after filtering out\n",
    "print(f\"Available training samples: {len(sampled_train_df)}\")\n",
    "print(f\"Available validation samples: {len(sampled_val_df)}\")\n",
    "print(f\"Available test samples: {len(sampled_test_df)}\")\n",
    "\n",
    "# Sample based on the available data\n",
    "#here min function is used. if suppose you want to train your model for 10K data then you need to write in 1st parameter as 10k. the 2nd parameter gigve the len of entire\n",
    "#data sample available. the minimum of two will be considered. if you want to load full sample then give any number that is higher than the available sample.\n",
    "n_train_samples = min(60000, len(sampled_train_df))\n",
    "n_val_samples = min(5000, len(sampled_val_df))\n",
    "n_test_samples = min(5000, len(sampled_test_df))\n",
    "\n",
    "#the so sampled data is then given to sample function that helps in giving random data samples for training and not in order. if you are training the model on same data sample \n",
    "#again again ,it may lead to overfitting. thats the reason randonstate is used. when you just change the number of random_state ,the new 10k samples will be taken(though if you dont change the saple size)\n",
    "sampled_train_df = sampled_train_df.sample(n=n_train_samples, random_state=123)\n",
    "sampled_val_df = sampled_val_df.sample(n=n_val_samples, random_state=123)\n",
    "sampled_test_df = sampled_test_df.sample(n=n_test_samples, random_state=123)\n",
    "\n",
    "# Convert DataFrames back to Hugging Face Dataset for easy accesibility\n",
    "train_data = Dataset.from_pandas(sampled_train_df)\n",
    "val_data = Dataset.from_pandas(sampled_val_df)\n",
    "test_data = Dataset.from_pandas(sampled_test_df)\n",
    "\n",
    "\n",
    "#the final sampled data is stored in the form of dataset dictionary for easy oragnizability and accessibiblity\n",
    "filtered_dataset = DatasetDict({\n",
    "    'train': train_data,\n",
    "    'validation': val_data,\n",
    "    'test': test_data\n",
    "})\n",
    "\n",
    "#this line of code helps in utilizing of cuda(gpu server) if available,so that cpu and ram usage will be less\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize the filtered dataset\n",
    "def tokenize_function(examples):\n",
    "    # Add a prompt before each code example\n",
    "    prompt = \"Summarize the following JavaScript code: \"\n",
    "    inputs = [prompt + code for code in examples['code']]#A prompt is added before each code example.\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    #The inputs list is tokenized using the tokenizer. Here, max_length=512 specifies the maximum length of the \n",
    "    # tokenized input, truncating longer inputs and padding shorter ones to the specified length.\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['docstring'], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    #Within the context of tokenizer being used as a target tokenizer, the docstrings in examples['docstring'] are \n",
    "    # tokenized. The max_length=128 specifies the maximum length for these tokenized docstrings, truncating or padding as needed\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    #The tokenized labels (docstrings) are assigned to the model_inputs dictionary under the key \"labels\". labels[\"input_ids\"]\n",
    "    #  contains the token IDs for the docstrings.\n",
    "    return model_inputs\n",
    "     #The function returns the model_inputs dictionary, which now includes tokenized inputs and their corresponding labels\n",
    "\n",
    "tokenized_datasets = filtered_dataset.map(tokenize_function, batched=True)\n",
    "#By using map function, each piece of code in the dataset is prefixed with a prompt, tokenized along with its \n",
    "# corresponding docstring, and stored in a structured format ready for training a model.\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/data/T5/result4',         # Directory where the model checkpoints and outputs will be saved\n",
    "    # resume_from_checkpoint=True,         # Uncomment this line to resume training from the last checkpoint if interrupted\n",
    "    evaluation_strategy=\"epoch\",           # Evaluate the model at the end of each epoch\n",
    "    save_strategy=\"epoch\",                 # Save a checkpoint at the end of each epoch\n",
    "    learning_rate=3e-5,                    # Learning rate for training\n",
    "    per_device_train_batch_size=32,        # Batch size per device (GPU/CPU) for training\n",
    "    per_device_eval_batch_size=16,         # Batch size per device (GPU/CPU) for evaluation\n",
    "    num_train_epochs=12,                   # Number of epochs to train the model\n",
    "    weight_decay=0.01,                     # Weight decay for optimization to prevent overfitting\n",
    "    load_best_model_at_end=True,           # Load the best model (based on evaluation metric) at the end of training\n",
    "    metric_for_best_model=\"eval_loss\",     # Metric to determine the best model during evaluation\n",
    "    greater_is_better=False,               # Indicates if a lower metric value is better (True for metrics like accuracy)\n",
    "    logging_dir='/data/T5/result4',        # Directory for storing training logs\n",
    "    logging_steps=10,                      # Log training information every 10 steps\n",
    "    fp16=True,                             # Use 16-bit (half-precision) training to reduce memory usage and speed up training\n",
    "    warmup_steps=500,                      # Number of steps for learning rate warmup\n",
    "    save_total_limit=3                     # Limit the total number of saved checkpoints to 3 (older ones will be deleted)\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                                        # The pre-trained model to be trained\n",
    "    args=training_args,                                 # Training arguments defined previously\n",
    "    train_dataset=tokenized_datasets['train'],          # Tokenized training dataset\n",
    "    eval_dataset=tokenized_datasets['validation'],      # Tokenized validation dataset\n",
    "    tokenizer=tokenizer,                                # Tokenizer used for processing the inputs and targets\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]  # Callback for early stopping if no improvement for 5 evaluation steps\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the pretrained model with our data\n",
    "model.save_pretrained('/data/T5/codeT54_54k')\n",
    "tokenizer.save_pretrained('/data/T5/codeT54_54k')\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "WFrYVXu2Ou-2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter out odd numbers\n",
      "@param {Array} arr\n",
      "@returns {Array}\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, RobertaTokenizer\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model_path = '/data/T5/codeT54_54k'\n",
    "tokenizer =  RobertaTokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "def generate_summary(code_snippet):\n",
    "    # Tokenize the input code snippet\n",
    "    inputs = tokenizer(code_snippet, return_tensors='pt', max_length=512, truncation=True, padding=\"max_length\")\n",
    "    # The tokenizer processes the input code snippet into tokens, returns tensors for PyTorch ('pt'),\n",
    "    # truncates the input if it's longer than 512 tokens, and pads it to ensure it's exactly 512 tokens long.\n",
    "\n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=128, num_beams=4, early_stopping=True)\n",
    "    # The model generates the summary for the tokenized input. \n",
    "    # 'max_length=128' limits the summary to 128 tokens. \n",
    "    # 'num_beams=4' uses beam search with 4 beams to improve the quality of the generated summary.\n",
    "    # 'early_stopping=True' stops the generation once all beams produce an end-of-sequence token.\n",
    "\n",
    "    # Decode the generated tokens into a readable string\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    # The generated token IDs are converted back into a readable string.\n",
    "    # 'skip_special_tokens=True' ensures that special tokens (like <s>, </s>) are not included in the output.\n",
    "\n",
    "    return summary\n",
    "    # Return the generated summary as a readable string.\n",
    "\n",
    "\n",
    "# Example usage\n",
    "code_snippet = \"\"\"\n",
    "function filter(arr) {\n",
    "    return arr.filter(num => num % 2 == 0);\n",
    "}\n",
    "\n",
    "const numbers = [1, 2, 3, 4, 5, 6];\n",
    "console.log(filter(numbers));  \n",
    "\"\"\"\n",
    "summary = generate_summary(code_snippet)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Summary: Add two numbers\n",
      "@param {number} a\n",
      "@param {number} b\n",
      "@returns {number}\n",
      "Extended Summary: Add two numbers\n",
      "@param {number} a\n",
      "@param {number} b\n",
      "@returns {number} The number of elements in the array.\n",
      "\n",
      "Returns: The array containing the elements of the given number, or null if no element is found. If the number is greater than or equal to 0, the element will be removed from the list. Otherwise, it will not be added to the Array.prototype.array.removeAll(element, true). Note that this method does not return an array, so it is not recommended to use it to remove elements from arrays. This method is only useful if you want to add an element to a list and then remove it from that list by calling the removeAll method on it. For more information, see Remove All Elements from a List. Returns: A list of all elements that have been removed. The elements are sorted by the order in which they were removed, with the first element being the last element removed and the second element the most recent element. Note: If you have multiple elements, you may need to call the Remove all method multiple times to ensure that all of them are removed at the same time. It is recommended that you only call it once for each element in your list, as this will cause an infinite loop. You can also use the deleteAll() method to delete a single element, which will remove all the remaining elements and return a new array that contains the removed elements. See also: Listing 11.1.2. Removing an Element From an Array Returns an empty array with no elements at all. An array is a collection of objects, each of which has its own set of properties. Each of these properties can be set to any value, and each property can have a value that is unique to that property. In this example, we have the following properties: a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, RobertaTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model_path = '/data/T5/codeT52_54k'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "def generate_summary(code_snippet):\n",
    "    # Add a prompt to the input code snippet\n",
    "    prompt = \"Summarize the following JavaScript code: \"\n",
    "    input_text = prompt + code_snippet\n",
    "\n",
    "    # Tokenize the input code snippet with the prompt\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=128, num_beams=4, early_stopping=True)\n",
    "\n",
    "    # Decode the generated tokens into a readable string\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "gpt2_model_name = \"gpt2\"\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# Add padding token to the tokenizer\n",
    "gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Load the GPT-2 model after adding the special token\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# Function to generate extended summary using GPT-2\n",
    "def extend_summary_with_gpt2(summary):\n",
    "    # Encode the input summary\n",
    "    inputs = gpt2_tokenizer.encode(summary, return_tensors='pt', padding=True, truncation=True)\n",
    "    # The GPT-2 tokenizer encodes the input summary into token IDs, \n",
    "    # returns tensors for PyTorch ('pt'), pads the input to ensure uniform length, and truncates if necessary.\n",
    "\n",
    "    # Create the attention mask\n",
    "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)\n",
    "    # The attention mask is created, with all values set to 1. This tells the model to pay attention to all tokens.\n",
    "\n",
    "    # Generate the extended summary\n",
    "    extended_summary_ids = gpt2_model.generate(\n",
    "        inputs,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=512,\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=gpt2_tokenizer.pad_token_id  # Use the tokenizer's pad token ID\n",
    "    )\n",
    "    # The GPT-2 model generates an extended summary:\n",
    "    # - 'max_length=512' sets the maximum length of the generated summary to 512 tokens.\n",
    "    # - 'num_beams=5' uses beam search with 5 beams to improve the quality of the generated text.\n",
    "    # - 'no_repeat_ngram_size=2' prevents repeating n-grams of size 2, enhancing diversity.\n",
    "    # - 'early_stopping=True' stops the generation once all beams produce an end-of-sequence token.\n",
    "    # - 'pad_token_id' ensures the generated sequence is padded appropriately.\n",
    "\n",
    "    # Decode the generated tokens into a readable string\n",
    "    extended_summary = gpt2_tokenizer.decode(extended_summary_ids[0], skip_special_tokens=True)\n",
    "    # The generated token IDs are converted back into a readable string.\n",
    "    # 'skip_special_tokens=True' ensures that special tokens (like <s>, </s>) are not included in the output.\n",
    "\n",
    "    return extended_summary\n",
    "    # Return the generated extended summary as a readable string.\n",
    "\n",
    "\n",
    "# Example usage with the summary generated by CodeT5\n",
    "code_snippet = \"\"\"\n",
    "function myfun(a,b){\n",
    "    let res= a+b;\n",
    "    return res;\n",
    "}\n",
    "console.log(myfun(8,2));\n",
    "}\n",
    "\"\"\"\n",
    "initial_summary = generate_summary(code_snippet)\n",
    "print(\"Initial Summary:\", initial_summary)\n",
    "\n",
    "extended_summary = extend_summary_with_gpt2(initial_summary)\n",
    "print(\"Extended Summary:\", extended_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from nltk->rouge_score) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from nltk->rouge_score) (4.66.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge_score \n",
    "#to calculate rouge metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the metrics for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nm788186/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/nm788186/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/nm788186/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 11.391771768619734\n",
      "ROUGE-1: 0.3331085483726109\n",
      "ROUGE-2: 0.12496958196753155\n",
      "ROUGE-L: 0.29456832641993935\n",
      "METEOR: 0.29657723348155335\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration,RobertaTokenizer\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_metric\n",
    "import evaluate\n",
    "import sacrebleu\n",
    "# Load your fine-tuned model and tokenizer\n",
    "model_path = '/data/T5/codeT51_54k'  # Path where your model is saved\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load your test dataset (replace this with actual loading code)\n",
    "# test_df = pd.read_csv('path/to/your/test_data.csv')\n",
    "\n",
    "# Sample 10 examples from the test dataset\n",
    "sampled_test_df = test_df.sample(n=100, random_state=123)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "sampled_test_data = Dataset.from_pandas(sampled_test_df)\n",
    "\n",
    "# Generate predictions for the selected samples\n",
    "def generate_predictions(model, tokenizer, dataset, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []  # Initialize a list to store the predictions\n",
    "    references = []  # Initialize a list to store the reference docstrings\n",
    "\n",
    "    for example in dataset:  # Iterate over each example in the dataset\n",
    "        input_text = example['code']  # Extract the code snippet from the example\n",
    "        reference_text = example['docstring']  # Extract the reference docstring from the example\n",
    "\n",
    "        # Tokenize the input code snippet and move it to the specified device (CPU/GPU)\n",
    "        input_ids = tokenizer.encode(input_text, padding=\"max_length\", truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "        # Generate the predicted summary without computing gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids,               # Tokenized input code snippet\n",
    "                max_length=512,          # Maximum length of the generated summary\n",
    "                num_beams=64,            # Use beam search with 64 beams for better quality\n",
    "                early_stopping=True,     # Stop generation when an end-of-sequence token is generated\n",
    "                length_penalty=10        # Penalize longer sequences to encourage shorter summaries\n",
    "            )\n",
    "\n",
    "        # Decode the generated tokens into a readable string\n",
    "        predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predictions.append(predicted_text)  # Add the predicted summary to the predictions list\n",
    "        references.append(reference_text)  # Add the reference docstring to the references list\n",
    "\n",
    "    return predictions, references  # Return the lists of predictions and references\n",
    "\n",
    "\n",
    "# Generate predictions\n",
    "predictions, references = generate_predictions(model, tokenizer, sampled_test_data, device)\n",
    "\n",
    "# Load metrics\n",
    "rouge_metric = load_metric(\"rouge\")  # Load the ROUGE metric for evaluation\n",
    "meteor_metric = evaluate.load(\"meteor\")  # Load the METEOR metric for evaluation\n",
    "\n",
    "# Compute ROUGE scores\n",
    "rouge_results = rouge_metric.compute(predictions=predictions, references=references)\n",
    "# Compute ROUGE scores by comparing predictions and references\n",
    "\n",
    "# Compute METEOR scores\n",
    "meteor_results = meteor_metric.compute(predictions=predictions, references=references)\n",
    "# Compute METEOR scores by comparing predictions and references\n",
    "\n",
    "# Compute BLEU scores\n",
    "bleu_results = sacrebleu.corpus_bleu(predictions, [references])\n",
    "# Compute BLEU scores by comparing predictions and references\n",
    "\n",
    "# Print results\n",
    "print(\"BLEU:\", bleu_results.score)\n",
    "# Print the BLEU score\n",
    "\n",
    "print(\"ROUGE-1:\", rouge_results['rouge1'].mid.fmeasure)\n",
    "# Print the ROUGE-1 F-measure score\n",
    "\n",
    "print(\"ROUGE-2:\", rouge_results['rouge2'].mid.fmeasure)\n",
    "# Print the ROUGE-2 F-measure score\n",
    "\n",
    "print(\"ROUGE-L:\", rouge_results['rougeL'].mid.fmeasure)\n",
    "# Print the ROUGE-L F-measure score\n",
    "\n",
    "print(\"METEOR:\", meteor_results['meteor'])\n",
    "# Print the METEOR score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies need to create user friendly interface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr  # Import the Gradio library for creating web interfaces\n",
    "\n",
    "# Define the output text box\n",
    "output_text = gr.Textbox()\n",
    "# Initialize a Gradio Textbox component for displaying the output summary\n",
    "\n",
    "# Create a Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=generate_summary,               # Function to generate the summary\n",
    "    inputs=\"textbox\",                  # Input component: a text box for user to enter code snippets\n",
    "    outputs=output_text,               # Output component: the text box defined above for displaying the summary\n",
    "    title=\"Automatic Code Summarizer for JavaScript\",  # Title of the web interface\n",
    "    description=\"This app can summarize your JavaScript code snippets in natural language\"  # Description of the web interface\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "demo.launch()\n",
    "# Launch the web interface so it can be accessed in a web browser\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFUdxiMoNLF7",
    "outputId": "3ff085f2-c64c-40cc-eb32-d2a9d20fa6f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9gUsJW1gNPAA",
    "outputId": "8ccd36ac-7bf3-4736-b5ad-18b7345f47a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pyarrow 16.1.0\n",
      "Uninstalling pyarrow-16.1.0:\n",
      "  Successfully uninstalled pyarrow-16.1.0\n",
      "Found existing installation: requests 2.32.3\n",
      "Uninstalling requests-2.32.3:\n",
      "  Successfully uninstalled requests-2.32.3\n",
      "Requirement already satisfied: transformers in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (4.41.2)\n",
      "Requirement already satisfied: datasets in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: torch in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: evaluate in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (0.4.2)\n",
      "Requirement already satisfied: filelock in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (3.15.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/nm788186/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/nm788186/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: requests, pyarrow\n",
      "Successfully installed pyarrow-16.1.0 requests-2.32.3\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y pyarrow requests\n",
    "\n",
    "%pip install transformers datasets torch evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q16SEx-6NQy-",
    "outputId": "f30ef856-4d4c-4795-acdd-43a5a5466e5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (0.32.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from accelerate) (2.3.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from accelerate) (0.23.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.15.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
      "Requirement already satisfied: networkx in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
      "Requirement already satisfied: requests in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "0.32.1\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate -U\n",
    "import accelerate\n",
    "print(accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZXCLhml6NUTu"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# Load your XGLUE JavaScript dataset\n",
    "# THE DATASET FROM CODE_X_GLUE IS ALREADY PREPROCESSED\n",
    "dataset = load_dataset('code_x_glue_ct_code_to_text', 'javascript')\n",
    "\n",
    "# Convert the dataset to Pandas DataFrames\n",
    "\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "val_df = pd.DataFrame(dataset['validation'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable for CUDA memory allocation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import torch\n",
    "\n",
    "# Your PyTorch code goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "u4-ff1k-Ncp-",
    "outputId": "d7601729-6bbf-443e-af92-c1ad9a9bc05e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (732 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer and model loaded successfully.\n",
      "Available training samples: 54770\n",
      "Available validation samples: 3669\n",
      "Available test samples: 3110\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920aff0e108342a8bebfa3989c73104d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54770 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41cf26302e34e43890652a994c26a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3669 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4a79cf5c334edf857e9095690ece42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058ff610d3d147a5a2907496a2abb528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54770 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442164c4e58a4ecd9a5acb440c238542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3669 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c212151567e4e0589e1a101a3f701ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11984' max='20544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11984/20544 1:35:04 < 1:07:55, 2.10 it/s, Epoch 7/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.496900</td>\n",
       "      <td>0.621673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.505100</td>\n",
       "      <td>0.621418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.497700</td>\n",
       "      <td>0.624025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.467600</td>\n",
       "      <td>0.632316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.463400</td>\n",
       "      <td>0.635072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.642205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.428800</td>\n",
       "      <td>0.648374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [230/230 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6214179396629333, 'eval_runtime': 34.6305, 'eval_samples_per_second': 105.947, 'eval_steps_per_second': 6.642, 'epoch': 7.0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer\n",
    "import torch\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Model name\n",
    "#load the model directly from Salesforce/codeT5 when you are training for the first time\n",
    "#If fine tuning the models again then write the model_name as the previously saved model\n",
    "model_name = \"/data/T5/codeT53_54k\"\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Initialize the model\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "print(\"Tokenizer and model loaded successfully.\") #to verify if the model is loaded succefully or not\n",
    "\n",
    "# Filter the datasets based on code_tokens_length\n",
    "\n",
    "#it filters the data that has the length of code tokens as more than 512 in train data. you can either take code colm(which contain the actual code snippet) and then encode it\n",
    "#  using tokenizer. OR even you can use codetokens colum to directly filter out the thing without encoding. Similarly it is done for test and validation data.\n",
    "train_df['code_tokens_length'] = train_df['code'].apply(lambda x: len(tokenizer.encode(x)))#tokenize the code snippet in train data\n",
    "sampled_train_df = train_df[train_df['code_tokens_length'] <= 512]# filter out the data by removing the data that have more than 512 tokens and store it in sampled_train_df\n",
    "\n",
    "val_df['code_tokens_length'] = val_df['code'].apply(lambda x: len(tokenizer.encode(x)))#same explanation as in train data\n",
    "sampled_val_df = val_df[val_df['code_tokens_length'] <= 512]\n",
    "\n",
    "test_df['code_tokens_length'] = test_df['code'].apply(lambda x: len(tokenizer.encode(x)))\n",
    "sampled_test_df = test_df[test_df['code_tokens_length'] <= 512]\n",
    "\n",
    "# Check the number of available rows after filtering out\n",
    "print(f\"Available training samples: {len(sampled_train_df)}\")\n",
    "print(f\"Available validation samples: {len(sampled_val_df)}\")\n",
    "print(f\"Available test samples: {len(sampled_test_df)}\")\n",
    "\n",
    "# Sample based on the available data\n",
    "#here min function is used. if suppose you want to train your model for 10K data then you need to write in 1st parameter as 10k. the 2nd parameter gigve the len of entire\n",
    "#data sample available. the minimum of two will be considered. if you want to load full sample then give any number that is higher than the available sample.\n",
    "n_train_samples = min(60000, len(sampled_train_df))\n",
    "n_val_samples = min(5000, len(sampled_val_df))\n",
    "n_test_samples = min(5000, len(sampled_test_df))\n",
    "\n",
    "#the so sampled data is then given to sample function that helps in giving random data samples for training and not in order. if you are training the model on same data sample \n",
    "#again again ,it may lead to overfitting. thats the reason randonstate is used. when you just change the number of random_state ,the new 10k samples will be taken(though if you dont change the saple size)\n",
    "sampled_train_df = sampled_train_df.sample(n=n_train_samples, random_state=123)\n",
    "sampled_val_df = sampled_val_df.sample(n=n_val_samples, random_state=123)\n",
    "sampled_test_df = sampled_test_df.sample(n=n_test_samples, random_state=123)\n",
    "\n",
    "# Convert DataFrames back to Hugging Face Dataset for easy accesibility\n",
    "train_data = Dataset.from_pandas(sampled_train_df)\n",
    "val_data = Dataset.from_pandas(sampled_val_df)\n",
    "test_data = Dataset.from_pandas(sampled_test_df)\n",
    "\n",
    "\n",
    "#the final sampled data is stored in the form of dataset dictionary for easy oragnizability and accessibiblity\n",
    "filtered_dataset = DatasetDict({\n",
    "    'train': train_data,\n",
    "    'validation': val_data,\n",
    "    'test': test_data\n",
    "})\n",
    "\n",
    "#this line of code helps in utilizing of cuda(gpu server) if available,so that cpu and ram usage will be less\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize the filtered dataset\n",
    "def tokenize_function(examples):\n",
    "    # Add a prompt before each code example\n",
    "    prompt = \"Summarize the following JavaScript code: \"\n",
    "    inputs = [prompt + code for code in examples['code']]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['docstring'], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = filtered_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/data/T5/result4',\n",
    "    # resume_from_checkpoint=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=12,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_dir='/data/T5/result4',\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    warmup_steps=500,\n",
    "    save_total_limit=3\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the pretrained model with our data\n",
    "model.save_pretrained('/data/T5/codeT54_54k')\n",
    "tokenizer.save_pretrained('/data/T5/codeT54_54k')\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "WFrYVXu2Ou-2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter out odd numbers\n",
      "@param {Array} arr\n",
      "@returns {Array}\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, RobertaTokenizer\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model_path = '/data/T5/codeT54_54k'\n",
    "tokenizer =  RobertaTokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "def generate_summary(code_snippet):\n",
    "    # Tokenize the input code snippet\n",
    "    inputs = tokenizer(code_snippet, return_tensors='pt', max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=128, num_beams=4, early_stopping=True)\n",
    "\n",
    "    # Decode the generated tokens into a readable string\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "code_snippet = \"\"\"\n",
    "function filter(arr) {\n",
    "    return arr.filter(num => num % 2 == 0);\n",
    "}\n",
    "\n",
    "const numbers = [1, 2, 3, 4, 5, 6];\n",
    "console.log(filter(numbers));  \n",
    "\"\"\"\n",
    "summary = generate_summary(code_snippet)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Summary: Add two numbers\n",
      "@param {number} a\n",
      "@param {number} b\n",
      "@returns {number}\n",
      "Extended Summary: Add two numbers\n",
      "@param {number} a\n",
      "@param {number} b\n",
      "@returns {number} The number of elements in the array.\n",
      "\n",
      "Returns: The array containing the elements of the given number, or null if no element is found. If the number is greater than or equal to 0, the element will be removed from the list. Otherwise, it will not be added to the Array.prototype.array.removeAll(element, true). Note that this method does not return an array, so it is not recommended to use it to remove elements from arrays. This method is only useful if you want to add an element to a list and then remove it from that list by calling the removeAll method on it. For more information, see Remove All Elements from a List. Returns: A list of all elements that have been removed. The elements are sorted by the order in which they were removed, with the first element being the last element removed and the second element the most recent element. Note: If you have multiple elements, you may need to call the Remove all method multiple times to ensure that all of them are removed at the same time. It is recommended that you only call it once for each element in your list, as this will cause an infinite loop. You can also use the deleteAll() method to delete a single element, which will remove all the remaining elements and return a new array that contains the removed elements. See also: Listing 11.1.2. Removing an Element From an Array Returns an empty array with no elements at all. An array is a collection of objects, each of which has its own set of properties. Each of these properties can be set to any value, and each property can have a value that is unique to that property. In this example, we have the following properties: a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, RobertaTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model_path = '/data/T5/codeT52_54k'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "def generate_summary(code_snippet):\n",
    "    # Add a prompt to the input code snippet\n",
    "    prompt = \"Summarize the following JavaScript code: \"\n",
    "    input_text = prompt + code_snippet\n",
    "\n",
    "    # Tokenize the input code snippet with the prompt\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=128, num_beams=4, early_stopping=True)\n",
    "\n",
    "    # Decode the generated tokens into a readable string\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "gpt2_model_name = \"gpt2\"\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# Add padding token to the tokenizer\n",
    "gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Load the GPT-2 model after adding the special token\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# Function to generate extended summary using GPT-2\n",
    "def extend_summary_with_gpt2(summary):\n",
    "    # Encode the input summary\n",
    "    inputs = gpt2_tokenizer.encode(summary, return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "    # Create the attention mask\n",
    "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)\n",
    "    \n",
    "    # Generate the extended summary\n",
    "    extended_summary_ids = gpt2_model.generate(\n",
    "        inputs,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=512,\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=gpt2_tokenizer.pad_token_id  # Use the tokenizer's pad token ID\n",
    "    )\n",
    "    \n",
    "    # Decode the generated tokens into a readable string\n",
    "    extended_summary = gpt2_tokenizer.decode(extended_summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return extended_summary\n",
    "\n",
    "# Example usage with the summary generated by CodeT5\n",
    "code_snippet = \"\"\"\n",
    "function myfun(a,b){\n",
    "    let res= a+b;\n",
    "    return res;\n",
    "}\n",
    "console.log(myfun(8,2));\n",
    "}\n",
    "\"\"\"\n",
    "initial_summary = generate_summary(code_snippet)\n",
    "print(\"Initial Summary:\", initial_summary)\n",
    "\n",
    "extended_summary = extend_summary_with_gpt2(initial_summary)\n",
    "print(\"Extended Summary:\", extended_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from nltk->rouge_score) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from nltk->rouge_score) (4.66.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nm788186/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/nm788186/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/nm788186/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 11.391771768619734\n",
      "ROUGE-1: 0.3331085483726109\n",
      "ROUGE-2: 0.12496958196753155\n",
      "ROUGE-L: 0.29456832641993935\n",
      "METEOR: 0.29657723348155335\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration,RobertaTokenizer\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_metric\n",
    "import evaluate\n",
    "import sacrebleu\n",
    "# Load your fine-tuned model and tokenizer\n",
    "model_path = '/data/T5/codeT51_54k'  # Path where your model is saved\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load your test dataset (replace this with actual loading code)\n",
    "# test_df = pd.read_csv('path/to/your/test_data.csv')\n",
    "\n",
    "# Sample 10 examples from the test dataset\n",
    "sampled_test_df = test_df.sample(n=100, random_state=123)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "sampled_test_data = Dataset.from_pandas(sampled_test_df)\n",
    "\n",
    "# Generate predictions for the selected samples\n",
    "def generate_predictions(model, tokenizer, dataset, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for example in dataset:\n",
    "        input_text = example['code']\n",
    "        reference_text = example['docstring']\n",
    "\n",
    "        input_ids = tokenizer.encode(input_text, padding=\"max_length\", truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(input_ids, max_length=512, num_beams=64, early_stopping=True, length_penalty=10)\n",
    "        \n",
    "        predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predictions.append(predicted_text)\n",
    "        references.append(reference_text)\n",
    "\n",
    "    return predictions, references\n",
    "\n",
    "# Generate predictions\n",
    "predictions, references = generate_predictions(model, tokenizer, sampled_test_data, device)\n",
    "\n",
    "# Load metrics\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "\n",
    "# Compute ROUGE scores\n",
    "rouge_results = rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Compute METEOR scores\n",
    "meteor_results = meteor_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "bleu_results=sacrebleu.corpus_bleu(predictions,[references])\n",
    "# Print results\n",
    "print(\"BLEU:\",bleu_results.score)\n",
    "print(\"ROUGE-1:\", rouge_results['rouge1'].mid.fmeasure)\n",
    "print(\"ROUGE-2:\", rouge_results['rouge2'].mid.fmeasure)\n",
    "print(\"ROUGE-L:\", rouge_results['rougeL'].mid.fmeasure)\n",
    "print(\"METEOR:\", meteor_results['meteor'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "output_text=gr.Textbox()\n",
    "demo = gr.Interface(fn=generate_summary,\n",
    "                   inputs=\"textbox\",\n",
    "                   outputs=output_text,\n",
    "                   title=\"Automatic Code Summarizer for Javascript\",\n",
    "                   description=\"This app can summarize your javascript code snippets in natural language\")\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
